    Our project was born out of a real-world scenario from Erin's employer. Erin works for a book publisher that sells books online. There has been a desire at her company to get a better idea of how her company's products are being sold online, how they are presented and what they are being sold for. With that information, the company can ensure that their product pages online have accurate information, and analyze the profit margins the retailer is making. To gather that information, we needed to use a list of her company's books provided by the company. Fortunately, since the list could be filtered before generating, there was no cleanup necessary. This list was saved as a CSV (lerner_catalog_data), and it included each book's title, ISBN, release date, and other unique identifier numbers. The CSV was read into a dataframe (book_data). A new dataframe was created (book_data_clean) without unnecessary columns, leaving just the ASIN numbers. The dataframe was saved to a new CSV (book_data_clean). To load up each book's product information page, we needed to build a URL to connect to each page. We were able to find a search by ISBN query (AISN and ISBN numbers are equal) which redirected to the product details page. We used a for loop to piece together the unique URL for each book. Each URL was added to a dataframe, which upon completion, was exported to a CSV (url_list). We then used BeautifulSoup and Splinter to scrape each book's desired information: title, sale price, list price, and product details. The for loop read the url_list CSV one line (URL) at a time and scraped all this information (including a secondary for loop to add the product details into its own dictionary). Once all the details from the scrapes were acquired, a final dictionary was compiled (book_entry), which was then input into a collection using the non-relational database MongoDB. The collection can be called using the db.metadata.find command.